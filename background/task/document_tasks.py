from background.celery import celery_app
import os
import time
import json
import logging
from datetime import datetime, timedelta
from typing import Dict, Any, Optional
from celery import chain, current_task
from celery.exceptions import SoftTimeLimitExceeded
import redis

# Redis ì—°ê²° (ì¤‘ê°„ ê²°ê³¼ ì €ì¥ìš©)
redis_client = redis.Redis(
    host=os.environ.get("REDIS_HOST", "localhost"),
    port=int(os.environ.get("REDIS_PORT", "6379")),
    db=2,  # ë©”ì¸ Celeryì™€ ë‹¤ë¥¸ DB ì‚¬ìš©
    decode_responses=True
)

# êµ¬ì¡°í™”ëœ ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DocumentProcessor:
    """ë¬¸ì„œ ì²˜ë¦¬ ìƒíƒœ ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    @staticmethod
    def save_progress(task_id: str, step: str, data: Dict[Any, Any], progress: int):
        """ì§„í–‰ë¥ ê³¼ ì¤‘ê°„ ê²°ê³¼ ì €ì¥"""
        progress_data = {
            "task_id": task_id,
            "current_step": step,
            "progress": progress,
            "timestamp": datetime.now().isoformat(),
            "data": data,
            "status": "processing"
        }
        redis_client.setex(f"progress:{task_id}", 3600, json.dumps(progress_data))
        logger.info(f"ì§„í–‰ë¥  ì €ì¥: {task_id} - {step} ({progress}%)")
    
    @staticmethod
    def get_progress(task_id: str) -> Optional[Dict]:
        """ì§„í–‰ë¥  ì¡°íšŒ"""
        data = redis_client.get(f"progress:{task_id}")
        return json.loads(data) if data else None
    
    @staticmethod
    def save_intermediate_result(task_id: str, step: str, result: Dict[Any, Any]):
        """ì¤‘ê°„ ê²°ê³¼ ì €ì¥ (ì¬ì‹œì‘ ê°€ëŠ¥í•˜ë„ë¡)"""
        key = f"intermediate:{task_id}:{step}"
        redis_client.setex(key, 7200, json.dumps(result))  # 2ì‹œê°„ ë³´ê´€
        logger.info(f"ì¤‘ê°„ ê²°ê³¼ ì €ì¥: {step} - {task_id}")
    
    @staticmethod
    def get_intermediate_result(task_id: str, step: str) -> Optional[Dict]:
        """ì¤‘ê°„ ê²°ê³¼ ì¡°íšŒ"""
        key = f"intermediate:{task_id}:{step}"
        data = redis_client.get(key)
        return json.loads(data) if data else None

def send_notification(task_id: str, step: str, status: str, message: str, data: Dict = None):
    """ì•Œë¦¼ ì‹œìŠ¤í…œ (ì´ë©”ì¼/ìŠ¬ë™)"""
    notification_data = {
        "task_id": task_id,
        "step": step,
        "status": status,
        "message": message,
        "timestamp": datetime.now().isoformat(),
        "data": data or {}
    }
    
    # ì‹¤ì œë¡œëŠ” ì´ë©”ì¼/ìŠ¬ë™ API í˜¸ì¶œ
    if status == "error":
        logger.error(f"ğŸš¨ ì•Œë¦¼: {message}")
        # send_slack_alert(notification_data)
        # send_email_alert(notification_data)
    elif status == "success":
        logger.info(f"âœ… ì•Œë¦¼: {message}")
        # send_slack_notification(notification_data)
    elif status == "warning":
        logger.warning(f"âš ï¸ ì•Œë¦¼: {message}")
    
    # ì•Œë¦¼ íˆìŠ¤í† ë¦¬ ì €ì¥
    redis_client.lpush(f"notifications:{task_id}", json.dumps(notification_data))
    redis_client.expire(f"notifications:{task_id}", 86400)  # 24ì‹œê°„ ë³´ê´€

@celery_app.task(
    bind=True,
    soft_time_limit=120,  # 2ë¶„ ì†Œí”„íŠ¸ íƒ€ì„ì•„ì›ƒ
    time_limit=180,       # 3ë¶„ í•˜ë“œ íƒ€ì„ì•„ì›ƒ
    autoretry_for=(Exception,),
    retry_kwargs={'max_retries': 3, 'countdown': 60}
)
def extract_text_advanced(self, file_path: str, resume_data: Dict = None):
    """1ë‹¨ê³„: ê³ ê¸‰ í…ìŠ¤íŠ¸ ì¶”ì¶œ (íƒ€ì„ì•„ì›ƒ, ë¡œê¹…, ì¬ì‹œì‘ ê°€ëŠ¥)"""
    task_id = self.request.id
    step_name = "í…ìŠ¤íŠ¸_ì¶”ì¶œ"
    
    try:
        logger.info(f"[{task_id}] {step_name} ì‹œì‘: {file_path}")
        
        # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
        DocumentProcessor.save_progress(task_id, step_name, {"file_path": file_path}, 0)
        
        # ì¬ì‹œì‘ ê°€ëŠ¥: ì´ì „ ê²°ê³¼ í™•ì¸
        if resume_data:
            logger.info(f"[{task_id}] ì¬ì‹œì‘ ëª¨ë“œ: ì´ì „ ë°ì´í„° ì‚¬ìš©")
            DocumentProcessor.save_progress(task_id, step_name, resume_data, 100)
            return resume_data
        
        # ì´ì „ ì¤‘ê°„ ê²°ê³¼ í™•ì¸
        intermediate = DocumentProcessor.get_intermediate_result(task_id, step_name)
        if intermediate:
            logger.info(f"[{task_id}] ì¤‘ê°„ ê²°ê³¼ ë°œê²¬: ì¬ì‚¬ìš©")
            DocumentProcessor.save_progress(task_id, step_name, intermediate, 100)
            return intermediate
        
        # íŒŒì¼ ì¡´ì¬ í™•ì¸
        if not os.path.exists(file_path):
            error_msg = f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}"
            send_notification(task_id, step_name, "error", error_msg)
            raise FileNotFoundError(error_msg)
        
        # íŒŒì¼ í¬ê¸° í™•ì¸
        file_size = os.path.getsize(file_path)
        logger.info(f"[{task_id}] íŒŒì¼ í¬ê¸°: {file_size:,} bytes")
        
        # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
        DocumentProcessor.save_progress(task_id, step_name, {"file_path": file_path, "file_size": file_size}, 25)
        
        # ì‹¤ì œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‘ì—… ì‹œë®¬ë ˆì´ì…˜
        extracted_text = f"ë¬¸ì„œ ë‚´ìš© from {os.path.basename(file_path)} (í¬ê¸°: {file_size} bytes)"
        
        # ì²˜ë¦¬ ì‹œê°„ ì‹œë®¬ë ˆì´ì…˜ (íƒ€ì„ì•„ì›ƒ í…ŒìŠ¤íŠ¸ìš©)
        for i in range(10):
            time.sleep(0.2)
            progress = 25 + (i + 1) * 7.5
            DocumentProcessor.save_progress(task_id, step_name, {
                "file_path": file_path,
                "file_size": file_size,
                "processing": f"ì²­í¬ {i+1}/10 ì²˜ë¦¬ ì¤‘"
            }, int(progress))
        
        result = {
            "file_path": file_path,
            "text": extracted_text,
            "char_count": len(extracted_text),
            "file_size": file_size,
            "extraction_timestamp": datetime.now().isoformat(),
            "step_completed": step_name
        }
        
        # ì¤‘ê°„ ê²°ê³¼ ì €ì¥
        DocumentProcessor.save_intermediate_result(task_id, step_name, result)
        DocumentProcessor.save_progress(task_id, step_name, result, 100)
        
        # ì„±ê³µ ì•Œë¦¼
        send_notification(task_id, step_name, "success", 
                         f"í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ: {len(extracted_text)} characters", result)
        
        logger.info(f"[{task_id}] {step_name} ì™„ë£Œ: {len(extracted_text)} characters")
        return result
        
    except SoftTimeLimitExceeded:
        error_msg = f"{step_name} íƒ€ì„ì•„ì›ƒ (120ì´ˆ ì´ˆê³¼)"
        send_notification(task_id, step_name, "error", error_msg)
        logger.error(f"[{task_id}] {error_msg}")
        raise
    except Exception as e:
        error_msg = f"{step_name} ì‹¤íŒ¨: {str(e)}"
        send_notification(task_id, step_name, "error", error_msg)
        logger.error(f"[{task_id}] {error_msg}")
        raise

@celery_app.task(
    bind=True, # 
    soft_time_limit=90,
    time_limit=120,
    autoretry_for=(Exception,),
    retry_kwargs={'max_retries': 3, 'countdown': 30}
)
def split_text_chunks_advanced(self, extract_result: Dict):
    """2ë‹¨ê³„: ê³ ê¸‰ í…ìŠ¤íŠ¸ ì²­í‚¹"""
    task_id = self.request.id
    step_name = "í…ìŠ¤íŠ¸_ì²­í‚¹"
    
    try:
        logger.info(f"[{task_id}] {step_name} ì‹œì‘")
        
        # ì´ì „ ë‹¨ê³„ ê²°ê³¼ ê²€ì¦
        if not extract_result or "text" not in extract_result:
            error_msg = "ì´ì „ ë‹¨ê³„ ê²°ê³¼ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤"
            send_notification(task_id, step_name, "error", error_msg)
            raise ValueError(error_msg)
        
        # ì§„í–‰ë¥  ì´ˆê¸°í™”
        DocumentProcessor.save_progress(task_id, step_name, extract_result, 0)
        
        # ì¬ì‹œì‘ ê°€ëŠ¥: ì¤‘ê°„ ê²°ê³¼ í™•ì¸
        intermediate = DocumentProcessor.get_intermediate_result(task_id, step_name)
        if intermediate:
            logger.info(f"[{task_id}] ì¤‘ê°„ ê²°ê³¼ ë°œê²¬: ì¬ì‚¬ìš©")
            DocumentProcessor.save_progress(task_id, step_name, intermediate, 100)
            return intermediate
        
        text = extract_result["text"]
        chunk_size = 500
        
        # ì²­í‚¹ ì²˜ë¦¬
        chunks = []
        total_length = len(text)
        
        for i in range(0, total_length, chunk_size):
            chunk = text[i:i + chunk_size]
            chunk_data = {
                "chunk_id": i // chunk_size,
                "content": chunk,
                "start_pos": i,
                "end_pos": min(i + chunk_size, total_length),
                "char_count": len(chunk)
            }
            chunks.append(chunk_data)
            
            # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
            progress = int((i + chunk_size) / total_length * 80) + 10
            DocumentProcessor.save_progress(task_id, step_name, {
                **extract_result,
                "processing": f"ì²­í¬ {len(chunks)} ìƒì„± ì¤‘",
                "chunks_created": len(chunks)
            }, min(progress, 90))
            
            time.sleep(0.1)  # ì²˜ë¦¬ ì‹œê°„ ì‹œë®¬ë ˆì´ì…˜
        
        result = {
            **extract_result,
            "chunks": chunks,
            "total_chunks": len(chunks),
            "chunking_timestamp": datetime.now().isoformat(),
            "step_completed": step_name
        }
        
        # ì¤‘ê°„ ê²°ê³¼ ì €ì¥
        DocumentProcessor.save_intermediate_result(task_id, step_name, result)
        DocumentProcessor.save_progress(task_id, step_name, result, 100)
        
        # ì„±ê³µ ì•Œë¦¼
        send_notification(task_id, step_name, "success", 
                         f"í…ìŠ¤íŠ¸ ì²­í‚¹ ì™„ë£Œ: {len(chunks)} chunks", 
                         {"chunk_count": len(chunks)})
        
        logger.info(f"[{task_id}] {step_name} ì™„ë£Œ: {len(chunks)} chunks")
        return result
        
    except Exception as e:
        error_msg = f"{step_name} ì‹¤íŒ¨: {str(e)}"
        send_notification(task_id, step_name, "error", error_msg)
        logger.error(f"[{task_id}] {error_msg}")
        raise

@celery_app.task(
    bind=True,
    soft_time_limit=300,  # 5ë¶„ (ì„ë² ë”© ìƒì„±ì€ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¼)
    time_limit=420,       # 7ë¶„
    autoretry_for=(Exception,),
    retry_kwargs={'max_retries': 2, 'countdown': 120}
)
def generate_embeddings_advanced(self, chunk_result: Dict):
    """3ë‹¨ê³„: ê³ ê¸‰ ì„ë² ë”© ìƒì„±"""
    task_id = self.request.id
    step_name = "ì„ë² ë”©_ìƒì„±"
    
    try:
        logger.info(f"[{task_id}] {step_name} ì‹œì‘")
        
        chunks = chunk_result.get("chunks", [])
        if not chunks:
            error_msg = "ì²­í¬ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤"
            send_notification(task_id, step_name, "error", error_msg)
            raise ValueError(error_msg)
        
        # ì§„í–‰ë¥  ì´ˆê¸°í™”
        DocumentProcessor.save_progress(task_id, step_name, chunk_result, 0)
        
        # ì¬ì‹œì‘ ê°€ëŠ¥: ì¤‘ê°„ ê²°ê³¼ í™•ì¸
        intermediate = DocumentProcessor.get_intermediate_result(task_id, step_name)
        if intermediate:
            logger.info(f"[{task_id}] ì¤‘ê°„ ê²°ê³¼ ë°œê²¬: ì¬ì‚¬ìš©")
            DocumentProcessor.save_progress(task_id, step_name, intermediate, 100)
            return intermediate
        
        # ì„ë² ë”© ìƒì„± (ì²­í¬ë³„)
        total_chunks = len(chunks)
        embedded_chunks = []
        
        for i, chunk in enumerate(chunks):
            # ì‹¤ì œë¡œëŠ” OpenAI API í˜¸ì¶œ
            # embedding = openai.embeddings.create(...)
            
            chunk_with_embedding = {
                **chunk,
                "embedding": f"embedding_vector_{chunk['chunk_id']}_{task_id[:8]}",
                "embedding_model": "text-embedding-3-small",
                "embedding_timestamp": datetime.now().isoformat()
            }
            embedded_chunks.append(chunk_with_embedding)
            
            # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
            progress = int((i + 1) / total_chunks * 80) + 10
            DocumentProcessor.save_progress(task_id, step_name, {
                **chunk_result,
                "processing": f"ì„ë² ë”© {i+1}/{total_chunks} ìƒì„± ì¤‘",
                "embeddings_created": len(embedded_chunks)
            }, progress)
            
            time.sleep(0.3)  # API í˜¸ì¶œ ì‹œë®¬ë ˆì´ì…˜
            
            # ê²½ê³ : ì²˜ë¦¬ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¬ëŠ” ê²½ìš°
            if i % 10 == 0 and i > 0:
                send_notification(task_id, step_name, "warning", 
                                f"ì„ë² ë”© ìƒì„± ì§„í–‰ ì¤‘: {i}/{total_chunks}")
        
        result = {
            **chunk_result,
            "chunks": embedded_chunks,
            "embeddings_generated": True,
            "embedding_count": len(embedded_chunks),
            "embedding_timestamp": datetime.now().isoformat(),
            "step_completed": step_name
        }
        
        # ì¤‘ê°„ ê²°ê³¼ ì €ì¥
        DocumentProcessor.save_intermediate_result(task_id, step_name, result)
        DocumentProcessor.save_progress(task_id, step_name, result, 100)
        
        # ì„±ê³µ ì•Œë¦¼
        send_notification(task_id, step_name, "success", 
                         f"ì„ë² ë”© ìƒì„± ì™„ë£Œ: {len(embedded_chunks)} embeddings", 
                         {"embedding_count": len(embedded_chunks)})
        
        logger.info(f"[{task_id}] {step_name} ì™„ë£Œ: {len(embedded_chunks)} embeddings")
        return result
        
    except SoftTimeLimitExceeded:
        error_msg = f"{step_name} íƒ€ì„ì•„ì›ƒ (300ì´ˆ ì´ˆê³¼)"
        send_notification(task_id, step_name, "error", error_msg)
        logger.error(f"[{task_id}] {error_msg}")
        raise
    except Exception as e:
        error_msg = f"{step_name} ì‹¤íŒ¨: {str(e)}"
        send_notification(task_id, step_name, "error", error_msg)
        logger.error(f"[{task_id}] {error_msg}")
        raise

@celery_app.task(
    bind=True,
    soft_time_limit=60,
    time_limit=90,
    autoretry_for=(Exception,),
    retry_kwargs={'max_retries': 3, 'countdown': 30}
)
def save_to_database_advanced(self, embedding_result: Dict):
    """4ë‹¨ê³„: ê³ ê¸‰ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥"""
    task_id = self.request.id
    step_name = "ë°ì´í„°ë² ì´ìŠ¤_ì €ì¥"
    
    try:
        logger.info(f"[{task_id}] {step_name} ì‹œì‘")
        
        chunks = embedding_result.get("chunks", [])
        if not chunks:
            error_msg = "ì„ë² ë”© ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤"
            send_notification(task_id, step_name, "error", error_msg)
            raise ValueError(error_msg)
        
        # ì§„í–‰ë¥  ì´ˆê¸°í™”
        DocumentProcessor.save_progress(task_id, step_name, embedding_result, 0)
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
        saved_ids = []
        total_chunks = len(chunks)
        
        for i, chunk in enumerate(chunks):
            # ì‹¤ì œ ë²¡í„° DB ì €ì¥ ë¡œì§
            doc_id = f"doc_{chunk['chunk_id']}_{task_id[:8]}_{int(time.time())}"
            
            # ì‹¤ì œë¡œëŠ” Pinecone, Weaviate, ChromaDB ë“±ì— ì €ì¥
            # vector_db.insert(doc_id, chunk['embedding'], chunk['content'])
            
            saved_ids.append(doc_id)
            
            # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
            progress = int((i + 1) / total_chunks * 80) + 10
            DocumentProcessor.save_progress(task_id, step_name, {
                **embedding_result,
                "processing": f"ì €ì¥ {i+1}/{total_chunks}",
                "saved_count": len(saved_ids)
            }, progress)
            
            time.sleep(0.1)  # DB ì €ì¥ ì‹œë®¬ë ˆì´ì…˜
        
        # ìµœì¢… ê²°ê³¼
        final_result = {
            "task_id": task_id,
            "file_path": embedding_result["file_path"],
            "status": "completed",
            "total_chunks": embedding_result["total_chunks"],
            "saved_document_ids": saved_ids,
            "processing_summary": {
                "char_count": embedding_result["char_count"],
                "chunk_count": embedding_result["total_chunks"],
                "embedding_count": embedding_result["embedding_count"],
                "saved_count": len(saved_ids)
            },
            "completion_timestamp": datetime.now().isoformat(),
            "step_completed": step_name,
            "pipeline_completed": True
        }
        
        # ìµœì¢… ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
        DocumentProcessor.save_progress(task_id, "ì™„ë£Œ", final_result, 100)
        
        # ìµœì¢… ì„±ê³µ ì•Œë¦¼
        send_notification(task_id, "íŒŒì´í”„ë¼ì¸_ì™„ë£Œ", "success", 
                         f"ì „ì²´ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ! ë¬¸ì„œ {len(saved_ids)}ê°œ ì €ì¥", 
                         final_result["processing_summary"])
        
        logger.info(f"[{task_id}] ì „ì²´ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ: {len(saved_ids)} documents")
        return final_result
        
    except Exception as e:
        error_msg = f"{step_name} ì‹¤íŒ¨: {str(e)}"
        send_notification(task_id, step_name, "error", error_msg)
        logger.error(f"[{task_id}] {error_msg}")
        raise

# ì§„í–‰ë¥  ì¶”ì  ì „ìš© í•¨ìˆ˜
def get_pipeline_progress(task_id: str) -> Dict:
    """íŒŒì´í”„ë¼ì¸ ì „ì²´ ì§„í–‰ë¥  ì¡°íšŒ"""
    progress_data = DocumentProcessor.get_progress(task_id)
    if not progress_data:
        return {"error": "ì§„í–‰ë¥  ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"}
    
    return {
        "task_id": task_id,
        "current_step": progress_data.get("current_step"),
        "overall_progress": progress_data.get("progress", 0),
        "status": progress_data.get("status"),
        "last_updated": progress_data.get("timestamp"),
        "details": progress_data.get("data", {})
    }

# ì•Œë¦¼ íˆìŠ¤í† ë¦¬ ì¡°íšŒ
def get_notification_history(task_id: str) -> list:
    """ì‘ì—…ì˜ ì•Œë¦¼ íˆìŠ¤í† ë¦¬ ì¡°íšŒ"""
    notifications = redis_client.lrange(f"notifications:{task_id}", 0, -1)
    return [json.loads(notif) for notif in notifications]

# ê³ ê¸‰ íŒŒì´í”„ë¼ì¸ (ëª¨ë“  ê¸°ëŠ¥ í¬í•¨)
def process_document_pipeline_advanced(file_path: str):
    """ê³ ê¸‰ ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ - íƒ€ì„ì•„ì›ƒ, ë¡œê¹…, ì¬ì‹œì‘, ì§„í–‰ë¥ , ì•Œë¦¼ ëª¨ë‘ í¬í•¨"""
    pipeline = chain(
        extract_text_advanced.s(file_path),
        split_text_chunks_advanced.s(),
        generate_embeddings_advanced.s(),
        save_to_database_advanced.s()
    )
    
    result = pipeline.apply_async()
    
    # ì´ˆê¸° ì§„í–‰ë¥  ì„¤ì •
    DocumentProcessor.save_progress(result.id, "íŒŒì´í”„ë¼ì¸_ì‹œì‘", {
        "file_path": file_path,
        "pipeline_id": result.id,
        "steps": ["í…ìŠ¤íŠ¸_ì¶”ì¶œ", "í…ìŠ¤íŠ¸_ì²­í‚¹", "ì„ë² ë”©_ìƒì„±", "ë°ì´í„°ë² ì´ìŠ¤_ì €ì¥"]
    }, 0)
    
    # ì‹œì‘ ì•Œë¦¼
    send_notification(result.id, "íŒŒì´í”„ë¼ì¸_ì‹œì‘", "success", 
                     f"ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹œì‘: {os.path.basename(file_path)}")
    
    return result

# ê¸°ì¡´ í˜¸í™˜ì„± ìœ ì§€
@celery_app.task
def split_document(file_path: str):
    """ê¸°ì¡´ API í˜¸í™˜ì„±ì„ ìœ„í•œ ë‹¨ìˆœ ì‘ì—…"""
    try:
        logger.info(f"ë‹¨ìˆœ ë¬¸ì„œ ë¶„í•  ì‘ì—…: {file_path}")
        time.sleep(2)
        return f"ë¬¸ì„œ ë¶„í•  ì™„ë£Œ: {os.path.basename(file_path)}"
    except Exception as e:
        raise e